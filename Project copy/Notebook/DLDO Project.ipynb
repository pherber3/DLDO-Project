{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Run if the kernel starts dying, notebook doesn't clear automatically\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined for flattening input image inside our model\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_propagation(model, initial_bound):\n",
    "    l, u = initial_bound\n",
    "    bounds = []\n",
    "    \n",
    "    for layer in model:\n",
    "        if isinstance(layer, Flatten):\n",
    "            l_ = Flatten()(l)\n",
    "            u_ = Flatten()(u)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            l_ = (layer.weight.clamp(min=0) @ l.t() + layer.weight.clamp(max=0) @ u.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "            u_ = (layer.weight.clamp(min=0) @ u.t() + layer.weight.clamp(max=0) @ l.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            l_ = (nn.functional.conv2d(l, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(u, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "            u_ = (nn.functional.conv2d(u, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(l, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) + \n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            l_ = l.clamp(min=0)\n",
    "            u_ = u.clamp(min=0)\n",
    "            \n",
    "        bounds.append((l_, u_))\n",
    "        l,u = l_, u_\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def form_milp(model, c, initial_bounds, bounds):\n",
    "    linear_layers = [(layer, bound) for layer, bound in zip(model,bounds) if isinstance(layer, nn.Linear)]\n",
    "    d = len(linear_layers)-1\n",
    "    \n",
    "    # create cvxpy variables\n",
    "    z = ([cp.Variable(layer.in_features) for layer, _ in linear_layers] + \n",
    "         [cp.Variable(linear_layers[-1][0].out_features)])\n",
    "    v = [cp.Variable(layer.out_features, boolean=True) for layer, _ in linear_layers[:-1]]\n",
    "    \n",
    "    # extract relevant matrices\n",
    "    W = [layer.weight.detach().cpu().numpy() for layer,_ in linear_layers]\n",
    "    b = [layer.bias.detach().cpu().numpy() for layer,_ in linear_layers]\n",
    "    l = [l[0].detach().cpu().numpy() for _, (l,_) in linear_layers]\n",
    "    u = [u[0].detach().cpu().numpy() for _, (_,u) in linear_layers]\n",
    "    l0 = initial_bound[0][0].view(-1).detach().cpu().numpy()\n",
    "    u0 = initial_bound[1][0].view(-1).detach().cpu().numpy()\n",
    "    \n",
    "    # add ReLU constraints\n",
    "    constraints = []\n",
    "    for i in range(len(linear_layers)-1):\n",
    "        constraints += [z[i+1] >= W[i] @ z[i] + b[i], \n",
    "                        z[i+1] >= 0,\n",
    "                        cp.multiply(v[i], u[i]) >= z[i+1],\n",
    "                        W[i] @ z[i] + b[i] >= z[i+1] + cp.multiply((1-v[i]), l[i])]\n",
    "    \n",
    "    # final linear constraint\n",
    "    constraints += [z[d+1] == W[d] @ z[d] + b[d]]\n",
    "    \n",
    "    # initial bound constraints\n",
    "    constraints += [z[0] >= l0, z[0] <= u0]\n",
    "    \n",
    "    return cp.Problem(cp.Minimize(c @ z[d+1]), constraints), (z, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train = datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in test_loader:\n",
    "    X,y = X.to(device), y.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, module_list):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.ModuleList(module_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        ''' Returns the Iterator object '''\n",
    "        return iter(self.model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.model[index]\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        self.model[idx] = value\n",
    "\n",
    "    def register_backward_hooks(self):\n",
    "        for module_pt in self.model:\n",
    "            if hasattr(module_pt,'register_masking_hooks'):\n",
    "                module_pt.register_masking_hooks()\n",
    "    \n",
    "    def unregister_backward_hooks(self):\n",
    "        for module_pt in self.model:\n",
    "            if hasattr(module_pt,'unregister_masking_hooks'):\n",
    "                module_pt.unregister_masking_hooks()    \n",
    "\n",
    "    def forward(self, x):\n",
    "        for module_pt in self.model:\n",
    "            x = module_pt(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class FullyConnectedBaselineModel(Model):\n",
    "    def __init__(self, name='FCBaseline', input_size=784, n_output_classes=10, n_channels=-1):\n",
    "        self.name = name\n",
    "        module_list = [\n",
    "            Flatten(), \n",
    "            nn.Linear(input_size, 50), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 20), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, n_output_classes)\n",
    "        ]\n",
    "\n",
    "        super(FullyConnectedBaselineModel, self).__init__(module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/patrickherbert/Documents/Classes/Spring 2020/DL in DO/Project/mip-for-ann-master/experiments/FCBaseline/lr_2/epoch_20/dataset_0/optimizer_2/exp_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                   [-1, 50]          39,250\n",
      "              ReLU-3                   [-1, 50]               0\n",
      "            Linear-4                   [-1, 20]           1,020\n",
      "              ReLU-5                   [-1, 20]               0\n",
      "            Linear-6                   [-1, 10]             210\n",
      "================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = PATH + 'FCBaseline.pt'\n",
    "model_info = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "model_state_dict = model_info.get(\"model_state_dict\")\n",
    "model = FullyConnectedBaselineModel()\n",
    "model.load_state_dict(model_state_dict)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the MaskedLinear class which we need to create in order to make a MaskedModel object and get the weights from that loaded into a normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, indices_mask=None):\n",
    "        \"\"\"initialization of masked linear layer\n",
    "        \n",
    "        Arguments:\n",
    "            nn {[type]} -- [description]\n",
    "            in_dim {int} -- size of input features to linear layer\n",
    "            out_dim {int} -- size of output features \n",
    "        \n",
    "        Keyword Arguments:\n",
    "            indices_mask {list} --  list of two lists containing indices for dimensions 0 and 1, used to create the mask, dimension 0 in_dim and dimension 1 out dim which is n neurons (default: {None})\n",
    "        \"\"\"        \n",
    "        super(MaskedLinear, self).__init__()\n",
    "        if indices_mask is not None:\n",
    "            self.mask_neurons(indices_mask)\n",
    "        self.name = 'linear'\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_features = self.linear.in_features\n",
    "        self.out_features = self.linear.out_features\n",
    "        self.output_size = self.out_features\n",
    "        self.input_size = self.in_features\n",
    "        self.handle = None\n",
    "        self.has_indices_mask = False\n",
    "\n",
    "    @staticmethod\n",
    "    def copy_layer(linear_layer, input_size=-1):\n",
    "        \"\"\"clone a pytorch linear layer\n",
    "        \n",
    "        Arguments:\n",
    "            linear_layer {nn.Linear} -- linear layer that would be cloned into MaskedLinear Object\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            input_size {int} -- size of input (default: {-1})\n",
    "        \n",
    "        Returns:\n",
    "            MaskedConv -- returns a cloned MaskedConv object of the pytorch layer\n",
    "        \"\"\"        \n",
    "        new_layer = MaskedLinear(\n",
    "            linear_layer.in_features, linear_layer.out_features)\n",
    "        # copy weights to the new layer\n",
    "        new_layer.linear.weight.data = copy.deepcopy(linear_layer.weight.data)\n",
    "        new_layer.linear.bias.data = copy.deepcopy(linear_layer.bias.data)\n",
    "        return new_layer\n",
    "\n",
    "    def mask_neurons(self, indices_mask):\n",
    "        \"\"\"mask neurons on the output dimension of the Linear layer based on input indices\n",
    "        \n",
    "        Arguments:\n",
    "            indices_mask {list} -- list of indices of parameters to be masked in Linear layer\n",
    "        \"\"\"        \n",
    "        if len(indices_mask) > 0:\n",
    "            self.has_indices_mask = True\n",
    "        self.mask = torch.zeros([self.out_features, self.in_features]).bool()\n",
    "        if indices_mask.ndim == 1:\n",
    "            self.mask[indices_mask, :] = 1\n",
    "        else:\n",
    "            self.mask[indices_mask] = 1  # create mask\n",
    "        self.mask_cached_neurons()\n",
    "\n",
    "    def mask_cached_neurons(self):\n",
    "        \"\"\"\n",
    "        set masked weights to zero\n",
    "        \"\"\"        \n",
    "        if self.has_indices_mask:\n",
    "            self.linear.weight.data[self.mask] = 0\n",
    "\n",
    "    def backward_hook(self, grad):\n",
    "        \"\"\"\n",
    "        a callback backward hook called by pytorch during gradient computation\n",
    "        \n",
    "        Arguments:\n",
    "            grad {tensor} -- gradients of the parameter that pytorch registered this hook on\n",
    "        \n",
    "        Returns:\n",
    "            [tensor] -- updated gradients\n",
    "        \"\"\"  \n",
    "        # Clone due to not being allowed to modify in-place gradients\n",
    "        out = grad.clone()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def register_masking_hooks(self):\n",
    "        \"\"\"\n",
    "        register backward hook on convolutional weights for backprop through a sparse model\n",
    "        \"\"\"   \n",
    "        if self.has_indices_mask and self.handle is None:\n",
    "            self.handle = self.linear.weight.register_hook(self.backward_hook)\n",
    "\n",
    "    def unregister_masking_hooks(self):\n",
    "        \"\"\"\n",
    "        unregister an existing masking hooks after finishing trainingbecause hooks would make predictions slower\n",
    "        \"\"\"  \n",
    "        if self.handle is not None:\n",
    "            self.handle.remove()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass on input\n",
    "        \n",
    "        Arguments:\n",
    "            input {Tensor} -- input image that will pass through linear layer\n",
    "        \n",
    "        Returns:\n",
    "            tensor -- Linear layer output\n",
    "        \"\"\"  \n",
    "        x = self.linear(x)\n",
    "        if self.has_indices_mask:\n",
    "            assert (self.linear.weight.data[self.mask] == 0).all()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a new masked model object to load this into so that we can get the linear weights out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedFCBM(Model):\n",
    "    def __init__(self, name='MaskedFCBM', input_size=784, n_output_classes=10, n_channels=-1):\n",
    "        self.name = name\n",
    "        module_list = [\n",
    "            Flatten(), \n",
    "            MaskedLinear(input_size, 50), \n",
    "            nn.ReLU(True),\n",
    "            MaskedLinear(50, 20), \n",
    "            nn.ReLU(True),\n",
    "            MaskedLinear(20, n_output_classes)\n",
    "        ]\n",
    "\n",
    "        super(MaskedFCBM, self).__init__(module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                   [-1, 50]          39,250\n",
      "      MaskedLinear-3                   [-1, 50]               0\n",
      "              ReLU-4                   [-1, 50]               0\n",
      "            Linear-5                   [-1, 20]           1,020\n",
      "      MaskedLinear-6                   [-1, 20]               0\n",
      "              ReLU-7                   [-1, 20]               0\n",
      "            Linear-8                   [-1, 10]             210\n",
      "      MaskedLinear-9                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MASK_PATH = PATH + 'FCBaseline_MASK_model_finetuned.pt'\n",
    "#MASK_PATH = PATH + 'FCBaseline_MASK.pt'\n",
    "mask_model_info = torch.load(MASK_PATH, map_location=torch.device('cpu'))\n",
    "mask_state_dict = mask_model_info.get(\"model_state_dict\")\n",
    "masked_model = MaskedFCBM()\n",
    "masked_model.load_state_dict(mask_state_dict)\n",
    "summary(masked_model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the linear part of the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = collections.OrderedDict()\n",
    "for name, param in masked_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        name = name.replace(\".linear\", \"\")\n",
    "        d[name] = param.data\n",
    "new_masked_model = FullyConnectedBaselineModel() # Loads masked weights into a normal FCBM model\n",
    "new_masked_model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can play around with different variable values for the MIP program and change values to mess with individual images and run them on the trained model. The index of the maximum value in the resulting array corresponds to the predicted image label. Here we should be able to see a slight difference in the predicted values for each of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1 # Change how much the pixels can change in order to create an adversarial example\n",
    "im_num = 2    # Change which test image we're accessing (i = 0,...,99)\n",
    "new_label = 5 # Set the label for which we want to find an adversarial example for im_num, 0 for 0 and so on up to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(1-X[im_num][0].cpu().numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2966, 12.4353,  0.8562, -2.1959,  1.8975,  0.5165, -5.8936,  0.6549,\n",
       "          1.1780, -1.6546]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = X[im_num:im_num+1][0]\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.2658,  11.9540,   0.3126,  -1.5795,   2.3502,  -1.1299,  -6.5017,\n",
       "           1.7842,   1.4604,  -1.0644]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_masked_model(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then set it up the problem to feed into Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bound = ((X[im_num:im_num + 1] - epsilon).clamp(min=0), (X[im_num:im_num + 1] + epsilon).clamp(max=1))\n",
    "bounds_m = bound_propagation(new_masked_model, initial_bound) # Change model to masked_model or vice versa\n",
    "bounds = bound_propagation(model, initial_bound) \n",
    "\n",
    "c = np.zeros(10)        # Creating the objective function for the MIP\n",
    "c[y[im_num].item()] = 1 # y is the correct label for the image selected, setting it to 1 minimzes it in the O.F.\n",
    "c[new_label] = -1       # Maximize the incorrect label we set above in the O.F.\n",
    "\n",
    "\n",
    "prob, (z, v) = form_milp(model, c, initial_bound, bounds) # Change model to masked_model or vice versa\n",
    "prob_m, (z_m, v_m) = form_milp(new_masked_model, c, initial_bound, bounds_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve MIP to find an adversarial example, if the O.F.V is negative then we have successfully found one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Changed value of parameter QCPDual to 1\n",
      "   Prev: 0  Min: 0  Max: 1  Default: 0\n",
      "Gurobi Optimizer version 9.0.1 build v9.0.1rc0 (mac64)\n",
      "Optimize a model with 1858 rows, 934 columns and 82598 nonzeros\n",
      "Model fingerprint: 0xf6d93b33\n",
      "Variable types: 864 continuous, 70 integer (70 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-06, 5e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e-03, 4e+01]\n",
      "Presolve removed 1649 rows and 10 columns\n",
      "Presolve time: 0.08s\n",
      "Presolved: 209 rows, 924 columns, 78358 nonzeros\n",
      "Variable types: 855 continuous, 69 integer (69 binary)\n",
      "\n",
      "Root relaxation: objective -1.204697e+02, 157 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 -120.46973    0   40          - -120.46973      -     -    0s\n",
      "     0     0 -116.38505    0   39          - -116.38505      -     -    0s\n",
      "     0     0 -112.83119    0   41          - -112.83119      -     -    0s\n",
      "     0     0 -112.82060    0   41          - -112.82060      -     -    0s\n",
      "     0     0 -105.06509    0   44          - -105.06509      -     -    0s\n",
      "H    0     0                     -24.4499875 -105.06509   330%     -    0s\n",
      "     0     0 -104.25305    0   45  -24.44999 -104.25305   326%     -    0s\n",
      "     0     0  -94.96989    0   44  -24.44999  -94.96989   288%     -    0s\n",
      "     0     0  -94.96989    0   44  -24.44999  -94.96989   288%     -    0s\n",
      "     0     0  -80.44102    0   47  -24.44999  -80.44102   229%     -    0s\n",
      "H    0     0                     -26.3739694  -80.44102   205%     -    0s\n",
      "H    0     0                     -26.9995349  -80.44102   198%     -    0s\n",
      "     0     0  -80.44102    0   47  -26.99953  -80.44102   198%     -    0s\n",
      "     0     0  -79.93427    0   49  -26.99953  -79.93427   196%     -    1s\n",
      "     0     0  -79.93427    0   50  -26.99953  -79.93427   196%     -    1s\n",
      "     0     0  -79.93427    0   50  -26.99953  -79.93427   196%     -    1s\n",
      "     0     0  -78.64161    0   49  -26.99953  -78.64161   191%     -    1s\n",
      "     0     0  -78.64161    0   49  -26.99953  -78.64161   191%     -    1s\n",
      "     0     2  -78.64161    0   49  -26.99953  -78.64161   191%     -    2s\n",
      "   468   283  -27.62915   14   18  -26.99953  -58.95946   118%   101    5s\n",
      "  1650   730  -33.08852   14   23  -26.99953  -45.40136  68.2%  84.4   10s\n",
      "* 1746   769              34     -27.0286717  -44.85134  65.9%  83.2   10s\n",
      "* 2315   832              36     -28.0307274  -42.88844  53.0%  80.2   12s\n",
      "  3019   981     cutoff   18       -28.03073  -41.13775  46.8%  76.0   15s\n",
      "* 3982  1115              48     -28.3157932  -40.87806  44.4%  65.3   19s\n",
      "  4361  1176  -29.64214   25   21  -28.31579  -40.87806  44.4%  62.1   20s\n",
      "H 4612  1138                     -28.4026187  -40.87806  43.9%  60.1   20s\n",
      "  9253  2153  -30.10404   28   19  -28.40262  -40.87806  43.9%  43.1   25s\n",
      " 16212  3272  -31.68704   34   17  -28.40262  -39.26935  38.3%  35.3   30s\n",
      " 20648  3637  -30.98329   28   49  -28.40262  -37.61820  32.4%  33.0   54s\n",
      " 20656  3642  -31.04830   33   48  -28.40262  -37.61820  32.4%  33.0   55s\n",
      " 20663  3647  -35.71856   29   46  -28.40262  -37.61820  32.4%  33.0   60s\n",
      " 20670  3652  -32.93554   33   46  -28.40262  -37.61820  32.4%  32.9   65s\n",
      " 20675  3655  -35.67373   29   46  -28.40262  -37.61820  32.4%  32.9   70s\n",
      " 20680  3658  -36.70997   32   52  -28.40262  -37.61820  32.4%  32.9   75s\n",
      " 20684  3661  -30.48041   33   53  -28.40262  -37.61820  32.4%  32.9   80s\n",
      " 20687  3663  -34.23082   30   52  -28.40262  -37.61820  32.4%  32.9   85s\n",
      " 20691  3666  -31.32105   33   52  -28.40262  -37.61820  32.4%  32.9   92s\n",
      " 20693  3667  -33.69442   32   52  -28.40262  -37.61820  32.4%  32.9   96s\n",
      " 20695  3668  -36.68719   32   52  -28.40262  -37.61820  32.4%  32.9  100s\n",
      " 20697  3670  -29.95453   34   52  -28.40262  -37.61820  32.4%  32.9  105s\n",
      " 20700  3672  -37.02115   27   52  -28.40262  -37.61820  32.4%  32.9  110s\n",
      " 20703  3674  -35.79868   32   52  -28.40262  -37.61820  32.4%  32.9  116s\n",
      " 20706  3676  -32.79453   35   52  -28.40262  -37.61820  32.4%  32.9  122s\n",
      " 20708  3677  -32.95417   30   52  -28.40262  -37.61820  32.4%  32.9  125s\n",
      " 20710  3678  -37.33163   26   53  -28.40262  -37.61820  32.4%  32.9  130s\n",
      " 20713  3680  -35.47342   34   53  -28.40262  -37.61820  32.4%  32.9  136s\n",
      " 20715  3682  -35.02819   34   53  -28.40262  -37.61820  32.4%  32.9  141s\n",
      " 20718  3684  -30.31308   38   53  -28.40262  -37.61820  32.4%  32.9  146s\n",
      " 20720  3685  -30.91753   26   53  -28.40262  -37.61820  32.4%  32.9  150s\n",
      " 20722  3686  -33.72562   31   52  -28.40262  -37.61820  32.4%  32.9  155s\n",
      " 20724  3688  -37.02749   28   51  -28.40262  -37.61820  32.4%  32.9  160s\n",
      " 20726  3689  -34.60635   31   52  -28.40262  -37.61820  32.4%  32.9  165s\n",
      " 20729  3691  -34.08595   34   52  -28.40262  -37.61820  32.4%  32.9  170s\n",
      " 20732  3693  -35.72431   32   52  -28.40262  -37.61820  32.4%  32.8  176s\n",
      " 20734  3694  -31.76496   36   52  -28.40262  -37.61820  32.4%  32.8  180s\n",
      " 20735  3695  -29.27799   36   53  -28.40262  -37.61820  32.4%  32.8  187s\n",
      " 20737  3696  -36.29355   29   52  -28.40262  -37.61820  32.4%  32.8  192s\n",
      " 20739  3698  -31.62649   35   52  -28.40262  -37.61820  32.4%  32.8  196s\n",
      " 20741  3699  -33.75926   30   53  -28.40262  -37.61820  32.4%  32.8  200s\n",
      " 20743  3700  -30.79689   37   53  -28.40262  -37.61820  32.4%  32.8  205s\n",
      " 20745  3702  -31.31554   32   53  -28.40262  -37.61820  32.4%  32.8  212s\n",
      " 20746  3702  -36.79122   27   52  -28.40262  -37.61820  32.4%  32.8  215s\n",
      " 20748  3704  -30.98329   28   52  -28.40262  -37.61820  32.4%  32.8  221s\n",
      " 20750  3705  -36.10199   31   52  -28.40262  -37.61820  32.4%  32.8  225s\n",
      " 20753  3707  -30.71954   33   52  -28.40262  -37.61820  32.4%  32.8  231s\n",
      " 20755  3708  -29.93082   39   52  -28.40262  -37.61820  32.4%  32.8  235s\n",
      " 20758  3710  -29.98086   34   53  -28.40262  -37.61820  32.4%  32.8  241s\n",
      " 20760  3712  -35.52058   32   53  -28.40262  -37.61820  32.4%  32.8  246s\n",
      " 20762  3713  -31.68703   33   53  -28.40262  -37.61820  32.4%  32.8  252s\n",
      " 20764  3714  -33.75615   35   53  -28.40262  -37.61820  32.4%  32.8  255s\n",
      " 20767  3716  -35.16707   32   53  -28.40262  -37.61820  32.4%  32.8  261s\n",
      " 20769  3718  -33.71063   31   53  -28.40262  -37.61820  32.4%  32.8  265s\n",
      " 20771  3719  -28.83342   32   53  -28.40262  -37.61820  32.4%  32.8  270s\n",
      " 20773  3720  -35.47645   33   52  -28.40262  -37.61820  32.4%  32.8  276s\n",
      " 20775  3722  -35.67373   29   53  -28.40262  -37.61820  32.4%  32.8  281s\n",
      " 20777  3723  -32.60357   38   53  -28.40262  -37.61820  32.4%  32.8  285s\n",
      " 20780  3725  -36.70997   32   53  -28.40262  -37.61820  32.4%  32.8  291s\n",
      " 20783  3727  -29.72683   38   52  -28.40262  -37.61820  32.4%  32.8  297s\n",
      " 20785  3728  -30.47130   34   52  -28.40262  -37.61820  32.4%  32.8  301s\n",
      " 20787  3730  -34.23082   30   52  -28.40262  -37.61820  32.4%  32.8  308s\n",
      " 20788  3730  -33.74636   37   52  -28.40262  -37.61820  32.4%  32.8  310s\n",
      " 20790  3732  -36.20477   32   52  -28.40262  -37.61820  32.4%  32.8  316s\n",
      " 20791  3732  -31.32105   33   52  -28.40262  -37.61820  32.4%  32.8  321s\n",
      " 20793  3734  -33.69442   32   52  -28.40262  -37.61820  32.4%  32.8  325s\n",
      " 20796  3736  -35.67372   31   52  -28.40262  -37.61820  32.4%  32.7  331s\n",
      " 20798  3737  -37.26904   29   52  -28.40262  -37.61820  32.4%  32.7  335s\n",
      " 20801  3739  -34.78784   30   52  -28.40262  -37.61820  32.4%  32.7  341s\n",
      " 20803  3740  -35.79868   32   52  -28.40262  -37.61820  32.4%  32.7  345s\n",
      " 20805  3742  -31.56678   34   52  -28.40262  -37.61820  32.4%  32.7  350s\n",
      " 20807  3743  -31.68610   33   52  -28.40262  -37.61820  32.4%  32.7  355s\n",
      " 20809  3744  -34.73122   33   52  -28.40262  -37.61820  32.4%  32.7  363s\n",
      " 20810  3745  -37.33163   26   52  -28.40262  -37.61820  32.4%  32.7  367s\n",
      " 20811  3746  -35.38951   24   52  -28.40262  -37.61820  32.4%  32.7  370s\n",
      " 20813  3747  -35.47342   34   52  -28.40262  -37.61820  32.4%  32.7  376s\n",
      " 20815  3748  -35.02819   34   52  -28.40262  -37.61820  32.4%  32.7  380s\n",
      " 20818  3750  -30.31308   38   52  -28.40262  -37.61820  32.4%  32.7  387s\n",
      " 20819  3751  -28.51689   32   52  -28.40262  -37.61820  32.4%  32.7  390s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20821  3752  -37.28388   26   52  -28.40262  -37.61820  32.4%  32.7  397s\n",
      " 20822  3753  -33.72562   31   52  -28.40262  -37.61820  32.4%  32.7  400s\n",
      " 20824  3754  -37.02749   28   52  -28.40262  -37.61820  32.4%  32.7  406s\n",
      " 20826  3756  -34.60635   31   52  -28.40262  -37.61820  32.4%  32.7  412s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Projected implied bound: 393\n",
      "  MIR: 111\n",
      "  Flow cover: 103\n",
      "  Relax-and-lift: 3\n",
      "\n",
      "Explored 20826 nodes (808488 simplex iterations) in 413.39 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 7: -28.4026 -28.3158 -28.0307 ... -24.45\n",
      "\n",
      "Solve interrupted\n",
      "Best objective -2.840261867857e+01, best bound -3.761820276055e+01, gap 32.4462%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-28.40261867857003"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.solve(solver=cp.GUROBI, verbose=True) # Solve normal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the perturbed image that we found to see what it looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(1-z[0].value.reshape(28,28), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Changed value of parameter QCPDual to 1\n",
      "   Prev: 0  Min: 0  Max: 1  Default: 0\n",
      "Gurobi Optimizer version 9.0.1 build v9.0.1rc0 (mac64)\n",
      "Optimize a model with 1858 rows, 934 columns and 79462 nonzeros\n",
      "Model fingerprint: 0xd2270a66\n",
      "Variable types: 864 continuous, 70 integer (70 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [6e-06, 5e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e-03, 4e+01]\n",
      "Presolve removed 1657 rows and 16 columns\n",
      "Presolve time: 0.09s\n",
      "Presolved: 201 rows, 918 columns, 75911 nonzeros\n",
      "Variable types: 851 continuous, 67 integer (67 binary)\n",
      "\n",
      "Root relaxation: objective -1.220040e+02, 165 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 -122.00402    0   39          - -122.00402      -     -    0s\n",
      "     0     0 -117.54048    0   39          - -117.54048      -     -    0s\n",
      "     0     0 -114.36576    0   37          - -114.36576      -     -    0s\n",
      "     0     0 -114.35770    0   37          - -114.35770      -     -    0s\n",
      "     0     0 -114.35770    0   37          - -114.35770      -     -    0s\n",
      "     0     0 -107.31638    0   42          - -107.31638      -     -    0s\n",
      "     0     0 -106.37078    0   44          - -106.37078      -     -    0s\n",
      "     0     0 -106.30326    0   43          - -106.30326      -     -    0s\n",
      "H    0     0                     -25.0737825 -106.30326   324%     -    0s\n",
      "     0     0 -106.30254    0   43  -25.07378 -106.30254   324%     -    0s\n",
      "     0     0  -80.25994    0   44  -25.07378  -80.25994   220%     -    0s\n",
      "     0     0  -80.25994    0   43  -25.07378  -80.25994   220%     -    1s\n",
      "     0     0  -80.25994    0   47  -25.07378  -80.25994   220%     -    1s\n",
      "     0     0  -80.25994    0   47  -25.07378  -80.25994   220%     -    1s\n",
      "     0     0  -80.25994    0   47  -25.07378  -80.25994   220%     -    1s\n",
      "H    0     0                     -26.3521556  -80.25994   205%     -    1s\n",
      "H    0     0                     -26.7096565  -80.25994   200%     -    1s\n",
      "     0     0  -79.37032    0   47  -26.70966  -79.37032   197%     -    1s\n",
      "     0     0  -79.37032    0   47  -26.70966  -79.37032   197%     -    2s\n",
      "     0     2  -79.37032    0   47  -26.70966  -79.37032   197%     -    2s\n",
      "   285   193  -27.25393   24    9  -26.70966  -64.84534   143%   132    5s\n",
      "  1064   448  -42.75826   10   29  -26.70966  -47.64129  78.4%   114   10s\n",
      "  1916   698  -26.82353   16   15  -26.70966  -43.95300  64.6%  97.7   15s\n",
      "  2965   979  -32.26469   12   21  -26.70966  -40.93774  53.3%  89.0   20s\n",
      "  3127   985  -32.06213   11   47  -26.70966  -40.81435  52.8%  87.6   25s\n",
      "  3159  1012  -33.75846   16   31  -26.70966  -40.81435  52.8%  90.8   30s\n",
      "H 3269  1011                     -26.9083351  -40.81435  51.7%  92.3   30s\n",
      "H 3316   982                     -27.5942462  -40.81435  47.9%  92.0   31s\n",
      "  4171   965  -38.85221   21   25  -27.59425  -40.81435  47.9%  90.8   35s\n",
      "  5341   791     cutoff   26       -27.59425  -40.81435  47.9%  86.2   40s\n",
      "  6348   765  -31.03716   21   21  -27.59425  -39.64774  43.7%  83.3   45s\n",
      "  7970   866  -32.63129   25   16  -27.59425  -37.59211  36.2%  78.2   50s\n",
      "  9759   994  -29.70923   29   17  -27.59425  -35.61626  29.1%  73.7   55s\n",
      " 11859   956  -27.84142   25   21  -27.59425  -34.16434  23.8%  69.2   61s\n",
      " 13966   794     cutoff   28       -27.59425  -32.78253  18.8%  65.8   65s\n",
      " 16135   539  -27.71482   32   14  -27.59425  -31.33327  13.6%  61.5   70s\n",
      "\n",
      "Cutting planes:\n",
      "  Projected implied bound: 49\n",
      "  MIR: 169\n",
      "  Flow cover: 23\n",
      "  Relax-and-lift: 3\n",
      "\n",
      "Explored 18455 nodes (1047372 simplex iterations) in 73.44 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: -27.5942 -26.9083 -26.7097 ... -25.0738\n",
      "No other solutions better than -27.5942\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective -2.759424622502e+01, best bound -2.759424622502e+01, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-27.59424622501823"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_m.solve(solver=cp.GUROBI, verbose=True) # Solve masked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(1-z_m[0].value.reshape(28,28), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the perturbed image through the trained model as a sanity check to validate the results we found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.5762,  -4.7535,  -6.8995,  10.0819,  -9.0895,  23.6491,   7.0851,\n",
       "           2.4542,   6.9296,  -2.5455]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_data = torch.tensor(z[0].value).float()\n",
    "perturbed_data = perturbed_data.view(1,28,-1)\n",
    "model(perturbed_data) # Print perturbed normal model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-13.1375,  -5.1264,  -7.7495,   7.9503,  -7.9970,  22.9326,   7.8789,\n",
       "           1.9800,   6.4039,  -1.8912]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_data = torch.tensor(z_m[0].value).float()\n",
    "perturbed_data = perturbed_data.view(1,28,-1)\n",
    "model(perturbed_data) # Print perturbed masked model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List:\n",
    "- Evalute accuracy of both models on the test set\n",
    "- Find subset of images that fool the pruned model but not the original model or maybe vice versa\n",
    "- Make models with different levels of sparsity and see how that affects the MIP result (difference of prob), MIP running time\n",
    "- Print out all the adversarial examples found by the MIP not just the optimal one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
