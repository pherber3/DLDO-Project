{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Run if the kernel starts dying, notebook doesn't clear automatically\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined for flattening input image inside our model\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_propagation(model, initial_bound):\n",
    "    l, u = initial_bound\n",
    "    bounds = []\n",
    "    \n",
    "    for layer in model:\n",
    "        if isinstance(layer, Flatten):\n",
    "            l_ = Flatten()(l)\n",
    "            u_ = Flatten()(u)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            l_ = (layer.weight.clamp(min=0) @ l.t() + layer.weight.clamp(max=0) @ u.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "            u_ = (layer.weight.clamp(min=0) @ u.t() + layer.weight.clamp(max=0) @ l.t() \n",
    "                  + layer.bias[:,None]).t()\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            l_ = (nn.functional.conv2d(l, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(u, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "            u_ = (nn.functional.conv2d(u, layer.weight.clamp(min=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) +\n",
    "                  nn.functional.conv2d(l, layer.weight.clamp(max=0), bias=None, \n",
    "                                       stride=layer.stride, padding=layer.padding,\n",
    "                                       dilation=layer.dilation, groups=layer.groups) + \n",
    "                  layer.bias[None,:,None,None])\n",
    "            \n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            l_ = l.clamp(min=0)\n",
    "            u_ = u.clamp(min=0)\n",
    "            \n",
    "        bounds.append((l_, u_))\n",
    "        l,u = l_, u_\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def form_milp(model, c, initial_bounds, bounds):\n",
    "    linear_layers = [(layer, bound) for layer, bound in zip(model,bounds) if isinstance(layer, nn.Linear)]\n",
    "    d = len(linear_layers)-1\n",
    "    \n",
    "    # create cvxpy variables\n",
    "    z = ([cp.Variable(layer.in_features) for layer, _ in linear_layers] + \n",
    "         [cp.Variable(linear_layers[-1][0].out_features)])\n",
    "    v = [cp.Variable(layer.out_features, boolean=True) for layer, _ in linear_layers[:-1]]\n",
    "    \n",
    "    # extract relevant matrices\n",
    "    W = [layer.weight.detach().cpu().numpy() for layer,_ in linear_layers]\n",
    "    b = [layer.bias.detach().cpu().numpy() for layer,_ in linear_layers]\n",
    "    l = [l[0].detach().cpu().numpy() for _, (l,_) in linear_layers]\n",
    "    u = [u[0].detach().cpu().numpy() for _, (_,u) in linear_layers]\n",
    "    l0 = initial_bound[0][0].view(-1).detach().cpu().numpy()\n",
    "    u0 = initial_bound[1][0].view(-1).detach().cpu().numpy()\n",
    "    \n",
    "    # add ReLU constraints\n",
    "    constraints = []\n",
    "    for i in range(len(linear_layers)-1):\n",
    "        constraints += [z[i+1] >= W[i] @ z[i] + b[i], \n",
    "                        z[i+1] >= 0,\n",
    "                        cp.multiply(v[i], u[i]) >= z[i+1],\n",
    "                        W[i] @ z[i] + b[i] >= z[i+1] + cp.multiply((1-v[i]), l[i])]\n",
    "    \n",
    "    # final linear constraint\n",
    "    constraints += [z[d+1] == W[d] @ z[d] + b[d]]\n",
    "    \n",
    "    # initial bound constraints\n",
    "    constraints += [z[0] >= l0, z[0] <= u0]\n",
    "    \n",
    "    return cp.Problem(cp.Minimize(c @ z[d+1]), constraints), (z, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train = datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in test_loader:\n",
    "    X,y = X.to(device), y.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, module_list):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.ModuleList(module_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        ''' Returns the Iterator object '''\n",
    "        return iter(self.model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.model[index]\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        self.model[idx] = value\n",
    "\n",
    "    def register_backward_hooks(self):\n",
    "        for module_pt in self.model:\n",
    "            if hasattr(module_pt,'register_masking_hooks'):\n",
    "                module_pt.register_masking_hooks()\n",
    "    \n",
    "    def unregister_backward_hooks(self):\n",
    "        for module_pt in self.model:\n",
    "            if hasattr(module_pt,'unregister_masking_hooks'):\n",
    "                module_pt.unregister_masking_hooks()    \n",
    "\n",
    "    def forward(self, x):\n",
    "        for module_pt in self.model:\n",
    "            x = module_pt(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class FullyConnectedBaselineModel(Model):\n",
    "    def __init__(self, name='FCBaseline', input_size=784, n_output_classes=10, n_channels=-1):\n",
    "        self.name = name\n",
    "        module_list = [\n",
    "            Flatten(), \n",
    "            nn.Linear(input_size, 50), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 20), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, n_output_classes)\n",
    "        ]\n",
    "\n",
    "        super(FullyConnectedBaselineModel, self).__init__(module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/patrickherbert/Documents/Classes/Spring 2020/DL in DO/Project/mip-for-ann-master/experiments/FCBaseline/lr_2/epoch_20/dataset_0/optimizer_2/exp_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                   [-1, 50]          39,250\n",
      "              ReLU-3                   [-1, 50]               0\n",
      "            Linear-4                   [-1, 20]           1,020\n",
      "              ReLU-5                   [-1, 20]               0\n",
      "            Linear-6                   [-1, 10]             210\n",
      "================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = PATH + 'FCBaseline.pt'\n",
    "model_info = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "model_state_dict = model_info.get(\"model_state_dict\")\n",
    "model = FullyConnectedBaselineModel()\n",
    "model.load_state_dict(model_state_dict)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the MaskedLinear class which we need to create in order to make a MaskedModel object and get the weights from that loaded into a normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, indices_mask=None):\n",
    "        \"\"\"initialization of masked linear layer\n",
    "        \n",
    "        Arguments:\n",
    "            nn {[type]} -- [description]\n",
    "            in_dim {int} -- size of input features to linear layer\n",
    "            out_dim {int} -- size of output features \n",
    "        \n",
    "        Keyword Arguments:\n",
    "            indices_mask {list} --  list of two lists containing indices for dimensions 0 and 1, used to create the mask, dimension 0 in_dim and dimension 1 out dim which is n neurons (default: {None})\n",
    "        \"\"\"        \n",
    "        super(MaskedLinear, self).__init__()\n",
    "        if indices_mask is not None:\n",
    "            self.mask_neurons(indices_mask)\n",
    "        self.name = 'linear'\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_features = self.linear.in_features\n",
    "        self.out_features = self.linear.out_features\n",
    "        self.output_size = self.out_features\n",
    "        self.input_size = self.in_features\n",
    "        self.handle = None\n",
    "        self.has_indices_mask = False\n",
    "\n",
    "    @staticmethod\n",
    "    def copy_layer(linear_layer, input_size=-1):\n",
    "        \"\"\"clone a pytorch linear layer\n",
    "        \n",
    "        Arguments:\n",
    "            linear_layer {nn.Linear} -- linear layer that would be cloned into MaskedLinear Object\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            input_size {int} -- size of input (default: {-1})\n",
    "        \n",
    "        Returns:\n",
    "            MaskedConv -- returns a cloned MaskedConv object of the pytorch layer\n",
    "        \"\"\"        \n",
    "        new_layer = MaskedLinear(\n",
    "            linear_layer.in_features, linear_layer.out_features)\n",
    "        # copy weights to the new layer\n",
    "        new_layer.linear.weight.data = copy.deepcopy(linear_layer.weight.data)\n",
    "        new_layer.linear.bias.data = copy.deepcopy(linear_layer.bias.data)\n",
    "        return new_layer\n",
    "\n",
    "    def mask_neurons(self, indices_mask):\n",
    "        \"\"\"mask neurons on the output dimension of the Linear layer based on input indices\n",
    "        \n",
    "        Arguments:\n",
    "            indices_mask {list} -- list of indices of parameters to be masked in Linear layer\n",
    "        \"\"\"        \n",
    "        if len(indices_mask) > 0:\n",
    "            self.has_indices_mask = True\n",
    "        self.mask = torch.zeros([self.out_features, self.in_features]).bool()\n",
    "        if indices_mask.ndim == 1:\n",
    "            self.mask[indices_mask, :] = 1\n",
    "        else:\n",
    "            self.mask[indices_mask] = 1  # create mask\n",
    "        self.mask_cached_neurons()\n",
    "\n",
    "    def mask_cached_neurons(self):\n",
    "        \"\"\"\n",
    "        set masked weights to zero\n",
    "        \"\"\"        \n",
    "        if self.has_indices_mask:\n",
    "            self.linear.weight.data[self.mask] = 0\n",
    "\n",
    "    def backward_hook(self, grad):\n",
    "        \"\"\"\n",
    "        a callback backward hook called by pytorch during gradient computation\n",
    "        \n",
    "        Arguments:\n",
    "            grad {tensor} -- gradients of the parameter that pytorch registered this hook on\n",
    "        \n",
    "        Returns:\n",
    "            [tensor] -- updated gradients\n",
    "        \"\"\"  \n",
    "        # Clone due to not being allowed to modify in-place gradients\n",
    "        out = grad.clone()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def register_masking_hooks(self):\n",
    "        \"\"\"\n",
    "        register backward hook on convolutional weights for backprop through a sparse model\n",
    "        \"\"\"   \n",
    "        if self.has_indices_mask and self.handle is None:\n",
    "            self.handle = self.linear.weight.register_hook(self.backward_hook)\n",
    "\n",
    "    def unregister_masking_hooks(self):\n",
    "        \"\"\"\n",
    "        unregister an existing masking hooks after finishing trainingbecause hooks would make predictions slower\n",
    "        \"\"\"  \n",
    "        if self.handle is not None:\n",
    "            self.handle.remove()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass on input\n",
    "        \n",
    "        Arguments:\n",
    "            input {Tensor} -- input image that will pass through linear layer\n",
    "        \n",
    "        Returns:\n",
    "            tensor -- Linear layer output\n",
    "        \"\"\"  \n",
    "        x = self.linear(x)\n",
    "        if self.has_indices_mask:\n",
    "            assert (self.linear.weight.data[self.mask] == 0).all()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a new masked model object to load this into so that we can get the linear weights out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedFCBM(Model):\n",
    "    def __init__(self, name='MaskedFCBM', input_size=784, n_output_classes=10, n_channels=-1):\n",
    "        self.name = name\n",
    "        module_list = [\n",
    "            Flatten(), \n",
    "            MaskedLinear(input_size, 50), \n",
    "            nn.ReLU(True),\n",
    "            MaskedLinear(50, 20), \n",
    "            nn.ReLU(True),\n",
    "            MaskedLinear(20, n_output_classes)\n",
    "        ]\n",
    "\n",
    "        super(MaskedFCBM, self).__init__(module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                   [-1, 50]          39,250\n",
      "      MaskedLinear-3                   [-1, 50]               0\n",
      "              ReLU-4                   [-1, 50]               0\n",
      "            Linear-5                   [-1, 20]           1,020\n",
      "      MaskedLinear-6                   [-1, 20]               0\n",
      "              ReLU-7                   [-1, 20]               0\n",
      "            Linear-8                   [-1, 10]             210\n",
      "      MaskedLinear-9                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MASK_PATH = PATH + 'FCBaseline_MASK_model_finetuned.pt'\n",
    "#MASK_PATH = PATH + 'FCBaseline_MASK.pt'\n",
    "mask_model_info = torch.load(MASK_PATH, map_location=torch.device('cpu'))\n",
    "mask_state_dict = mask_model_info.get(\"model_state_dict\")\n",
    "masked_model = MaskedFCBM()\n",
    "masked_model.load_state_dict(mask_state_dict)\n",
    "summary(masked_model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the linear part of the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = collections.OrderedDict()\n",
    "for name, param in masked_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        name = name.replace(\".linear\", \"\")\n",
    "        d[name] = param.data\n",
    "new_masked_model = FullyConnectedBaselineModel() # Loads masked weights into a normal FCBM model\n",
    "new_masked_model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can play around with different variable values for the MIP program and change values to mess with individual images and run them on the trained model. The index of the maximum value in the resulting array corresponds to the predicted image label. Here we should be able to see a slight difference in the predicted values for each of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10 # Change how much the pixels can change in order to create an adversarial example\n",
    "im_num = 2    # Change which test image we're accessing (i = 0,...,99)\n",
    "new_label = 5 # Set the label for which we want to find an adversarial example for im_num, 0 for 0 and so on up to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(1-X[im_num][0].cpu().numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2966, 12.4353,  0.8562, -2.1959,  1.8975,  0.5165, -5.8936,  0.6549,\n",
       "          1.1780, -1.6546]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = X[im_num:im_num+1][0]\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.2658,  11.9540,   0.3126,  -1.5795,   2.3502,  -1.1299,  -6.5017,\n",
       "           1.7842,   1.4604,  -1.0644]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_masked_model(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then set it up the problem to feed into Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bound = ((X[im_num:im_num + 1] - epsilon).clamp(min=0), (X[im_num:im_num + 1] + epsilon).clamp(max=1))\n",
    "bounds_m = bound_propagation(new_masked_model, initial_bound) # Change model to masked_model or vice versa\n",
    "bounds = bound_propagation(model, initial_bound) \n",
    "\n",
    "c = np.zeros(10)        # Creating the objective function for the MIP\n",
    "c[y[im_num].item()] = 1 # y is the correct label for the image selected, setting it to 1 minimzes it in the O.F.\n",
    "c[new_label] = -1       # Maximize the incorrect label we set above in the O.F.\n",
    "\n",
    "\n",
    "prob, (z, v) = form_milp(model, c, initial_bound, bounds) # Change model to masked_model or vice versa\n",
    "prob_m, (z_m, v_m) = form_milp(new_masked_model, c, initial_bound, bounds_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve MIP to find an adversarial example, if the O.F.V is negative then we have successfully found one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Changed value of parameter QCPDual to 1\n",
      "   Prev: 0  Min: 0  Max: 1  Default: 0\n",
      "Gurobi Optimizer version 9.0.1 build v9.0.1rc0 (mac64)\n",
      "Optimize a model with 1858 rows, 934 columns and 82598 nonzeros\n",
      "Model fingerprint: 0xc361a23e\n",
      "Variable types: 864 continuous, 70 integer (70 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-06, 2e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e-03, 1e+01]\n",
      "Presolve removed 1708 rows and 44 columns\n",
      "Presolve time: 0.18s\n",
      "Presolved: 150 rows, 890 columns, 51233 nonzeros\n",
      "Variable types: 846 continuous, 44 integer (44 binary)\n",
      "\n",
      "Root relaxation: objective -2.199283e+01, 95 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  -21.99283    0   22          -  -21.99283      -     -    0s\n",
      "     0     0  -10.03922    0   23          -  -10.03922      -     -    0s\n",
      "H    0     0                       0.0810944  -10.03922      -     -    0s\n",
      "     0     0  -10.02150    0   23    0.08109  -10.02150      -     -    0s\n",
      "H    0     0                      -0.0571115  -10.02150      -     -    0s\n",
      "     0     0   -8.72127    0   25   -0.05711   -8.72127      -     -    0s\n",
      "     0     0   -8.62228    0   25   -0.05711   -8.62228      -     -    0s\n",
      "H    0     0                      -0.4137388   -6.50979  1473%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 13\n",
      "  Implied bound: 2\n",
      "  MIR: 22\n",
      "  Flow cover: 22\n",
      "  RLT: 1\n",
      "\n",
      "Explored 1 nodes (185 simplex iterations) in 0.65 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: -0.413739 -0.0571115 0.0810944 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective -4.137387674735e-01, best bound -4.137387674735e-01, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.413738767473542"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.solve(solver=cp.GUROBI, verbose=True) # Solve normal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the perturbed image that we found to see what it looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOiklEQVR4nO3dYYhd9ZnH8d9vY/LG9EWyN7ohDbFboqwsrNYxCFmKUi1GhFiwq3lRUgybvlBpoS82uC/qK5HFtvTFWkhXbbp0DcU2GkF2K6EogVCcaDbGDa5ZiW3qmMmQF7UoZhOffTEnyxjnnjve/z33nJnn+4FwZ+7/nnOeOXN/OXfuc8/5OyIEYOn7s7YLADAehB1IgrADSRB2IAnCDiRx2Tg31uv1YsOGDUMvb3voZUu7DnXbHrTuQXWXLl+y7kFKa29Tk7+zQZpefz8nT57UzMzMvCsvCrvt2yX9SNIySf8SEY/WPX7Dhg06dOjQ0NtbsWLF0MueO3du6GUHbXvQugfVXbp8yboHKa29TU3+zgZpev39TExM9B0b+mW87WWS/lnSFknXStpm+9ph1wegWSV/s2+SdCIi3o6Ic5L2Sto6mrIAjFpJ2NdJ+v2c709V932C7Z22J21PzszMFGwOQImSsM/3JsCn3pWIiN0RMRERE71er2BzAEqUhP2UpPVzvv+8pHfLygHQlJKwvyJpo+0v2F4h6V5J+0dTFoBRG7r1FhHnbT8g6T8023p7MiLeGFll86hrZ5S2MppsMZW2p9psbzXdVizRVPtKar5l2YaiPntEvCDphRHVAqBBfFwWSIKwA0kQdiAJwg4kQdiBJAg7kMRYz2e3XXTaYZPa3HaTveq2+71t9sLb/tmH1dTPxZEdSIKwA0kQdiAJwg4kQdiBJAg7kMRYW2+l2mylNHmF1ybbfm2fotpkq3WptixLfq66S1hzZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJMbaZ4+Ixi4H3eWebZuavsT2IO+8807fsauuuqp22ccee6x2/MEHH6wdL+nxL9bfdx2O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxKK6lHTdsov1/OOl7rXXXht62XXr1tWOl+z3jL+zorDbPinpfUkXJJ2PiIlRFAVg9EZxZL8lImZGsB4ADeJvdiCJ0rCHpF/bPmx753wPsL3T9qTtyTNnzhRuDsCwSsO+OSK+JGmLpPttf/nSB0TE7oiYiIiJNWvWFG4OwLCKwh4R71a305L2Sdo0iqIAjN7QYbd9ue3PXfxa0lclHRtVYQBGq+Td+Csl7bN9cT3/FhH/XlJMl/umi3X637bV9dl7vV7tsvfee2/teMnvdDH30euei1Ue5zV02CPibUl/M+zyAMaL1huQBGEHkiDsQBKEHUiCsANJdOpS0iWabqUs5lZNkw4fPlw7/sgjj/Qd2759e+2yTe7zjKe4cmQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTG2mfH0vPmm2/Wjp8/f77v2D333DPqchZsMffR62qPiL5jHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIlF1Wcvme55KWvzMtePP/547fjVV1/dd+yWW26pXfayy8qenpmfE/PhyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSSyqPntXp2wetO6mayvZL4NMTU3Vjk9OTtaOb9y4se9YaR89q2GnbB54ZLf9pO1p28fm3Lfa9ou236puV33WggGM10Jexv9U0u2X3LdL0oGI2CjpQPU9gA4bGPaIeFnS2Uvu3ippT/X1Hkl3jbguACM27Bt0V0bElCRVt1f0e6DtnbYnbU/OzMwMuTkApRp/Nz4idkfERERM9Hq9pjcHoI9hw37a9lpJqm6nR1cSgCYMG/b9ki7Ot7td0nOjKQdAUwY2Om0/LelmST3bpyR9T9Kjkn5he4ek30n6epNFXtTm+ewl62/zvOrSzwC89NJLRdtfs2ZN37GmP5/Q5c9GlNQ27HXjB4Y9Irb1GfrKoGUBdAcflwWSIOxAEoQdSIKwA0kQdiCJRXWOYUm7o8nLLXf5ksWlP/fRo0eLlt+1q/85Uk1fArvJU6LbvHx3Y6e4AlgaCDuQBGEHkiDsQBKEHUiCsANJEHYgibH22W03dppqad+zy73yEoN+rmPHjtWOP/XUU7Xj119/fe34bbfdVju+WDXZh2+qh8+RHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSGGufPSKWbD97sXr22Wdrx8+evXSav0/avHlz7Xjd+dVdvsR2m+unzw6gCGEHkiDsQBKEHUiCsANJEHYgCcIOJLGorhtf0n9ss6fb9Ln2JdcIOHHiRO14XZ9cku67777accyvjevODzyy237S9rTtY3Pue9j2H2wfqf7d0WyZAEot5GX8TyXdPs/9P4yI66p/L4y2LACjNjDsEfGypPrPTALovJI36B6wfbR6mb+q34Ns77Q9aXtyZmamYHMASgwb9h9L+qKk6yRNSfp+vwdGxO6ImIiIiV6vN+TmAJQaKuwRcToiLkTEx5J+ImnTaMsCMGpDhd322jnffk1S/fWIAbRuYJ/d9tOSbpbUs31K0vck3Wz7Okkh6aSkb42imC730Zu63n3T3nvvvdrxgwcP1o5fc801teO33npr7XhX903p/Oyl62/DwLBHxLZ57n6igVoANIiPywJJEHYgCcIOJEHYgSQIO5BEp6ZsblOTrZg2L1v8zDPP1C47PT1dO75ly5ba8aZbWG1p83na1HTQHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIlOTdlc0rNdqv3ehaj72Y8dK7vUwPLly2vHm9yvTV+Cu0klvXKmbAZQhLADSRB2IAnCDiRB2IEkCDuQBGEHkujU+ext9mwXcx++rvbnn3++aN133nln0fJ1Sn8nXb02QldxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJMbaZ0czjhw50nfsgw8+qF32o48+qh1fuXLlUDVdVNILL+2jN9mHX4yfARh4ZLe93vZvbB+3/Ybtb1f3r7b9ou23qttVzZcLYFgLeRl/XtJ3I+KvJN0k6X7b10raJelARGyUdKD6HkBHDQx7RExFxKvV1+9LOi5pnaStkvZUD9sj6a6migRQ7jO9QWf7KknXS/qtpCsjYkqa/Q9B0hV9ltlpe9L25JkzZ8qqBTC0BYfd9kpJv5T0nYj440KXi4jdETERERNr1qwZpkYAI7CgsNtertmg/zwiflXdfdr22mp8raT66UABtGpg6822JT0h6XhE/GDO0H5J2yU9Wt0+V1pMm+2Kxdym2bdvX9+xCxcu1C5700031Y7fcMMNteNNnjrcxfZVF9Tt04joO7aQPvtmSd+Q9Lrtiw3dhzQb8l/Y3iHpd5K+vtBiAYzfwLBHxEFJ7jP8ldGWA6ApfFwWSIKwA0kQdiAJwg4kQdiBJJbMKa5dPuWwdNuDeuV79+7tOzboFNa77767dnzZsmW14yX7fTFfvrtJJft09mMx8+PIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJjLXPHhGtnd/cZk+3tM/+4Ycf1o6vXr2679iNN95Yu+yOHTtqx7Oer97086Vu/U3tF47sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEp85n73LftURpz3bQfjl06FDR+uuU1t5kv7rkMwCDlm36udjUZ0bqrhvPkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkljI/OzrJf1M0l9I+ljS7oj4ke2HJf29pDPVQx+KiBeaKrRUad+0yfO2uX76cEr2W9P7vMk+/bDXjV/Ih2rOS/puRLxq+3OSDtt+sRr7YUQ89lkKBdCOhczPPiVpqvr6fdvHJa1rujAAo/WZ/ma3fZWk6yX9trrrAdtHbT9pe1WfZXbanrQ9OTMzU1QsgOEtOOy2V0r6paTvRMQfJf1Y0hclXafZI//351suInZHxERETPR6vRGUDGAYCwq77eWaDfrPI+JXkhQRpyPiQkR8LOknkjY1VyaAUgPD7tm3956QdDwifjDn/rVzHvY1ScdGXx6AUVnIu/GbJX1D0uu2j1T3PSRpm+3rJIWkk5K+VVrMUp52ebEqbRs2ud+W6mWsm7KQd+MPSpqvedfZnjqAT+MTdEAShB1IgrADSRB2IAnCDiRB2IEkuJR0pcs9/hJNn17b5DTabV/uuUTJz9bUc5EjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k4bopXke+MfuMpHfm3NWT1NUL03W1tq7WJVHbsEZZ24aIWDPfwFjD/qmN25MRMdFaATW6WltX65KobVjjqo2X8UAShB1Iou2w7255+3W6WltX65KobVhjqa3Vv9kBjE/bR3YAY0LYgSRaCbvt222/afuE7V1t1NCP7ZO2X7d9xPZky7U8aXva9rE59622/aLtt6rbeefYa6m2h23/odp3R2zf0VJt623/xvZx22/Y/nZ1f6v7rqausey3sf/NbnuZpP+WdJukU5JekbQtIv5rrIX0YfukpImIaP0DGLa/LOlPkn4WEX9d3fdPks5GxKPVf5SrIuIfOlLbw5L+1PY03tVsRWvnTjMu6S5J31SL+66mrr/TGPZbG0f2TZJORMTbEXFO0l5JW1uoo/Mi4mVJZy+5e6ukPdXXezT7ZBm7PrV1QkRMRcSr1dfvS7o4zXir+66mrrFoI+zrJP1+zven1K353kPSr20ftr2z7WLmcWVETEmzTx5JV7Rcz6UGTuM9TpdMM96ZfTfM9Oel2gj7fFNJdan/tzkiviRpi6T7q5erWJgFTeM9LvNMM94Jw05/XqqNsJ+StH7O95+X9G4LdcwrIt6tbqcl7VP3pqI+fXEG3ep2uuV6/l+XpvGeb5pxdWDftTn9eRthf0XSRttfsL1C0r2S9rdQx6fYvrx640S2L5f0VXVvKur9krZXX2+X9FyLtXxCV6bx7jfNuFred61Pfx4RY/8n6Q7NviP/P5L+sY0a+tT1l5L+s/r3Rtu1SXpasy/r/lezr4h2SPpzSQckvVXdru5Qbf8q6XVJRzUbrLUt1fa3mv3T8KikI9W/O9redzV1jWW/8XFZIAk+QQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfFz9fu2x/O0UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(1-z[0].value.reshape(28,28), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Problem\n",
      "  Name                   :                 \n",
      "  Objective sense        : min             \n",
      "  Type                   : LO (linear optimization problem)\n",
      "  Constraints            : 1858            \n",
      "  Cones                  : 0               \n",
      "  Scalar variables       : 934             \n",
      "  Matrix variables       : 0               \n",
      "  Integer variables      : 70              \n",
      "\n",
      "Optimizer started.\n",
      "Mixed integer optimizer started.\n",
      "Threads used: 8\n",
      "Presolve started.\n",
      "Presolve terminated. Time = 0.03\n",
      "Presolved problem: 887 variables, 152 constraints, 54286 non-zeros\n",
      "Presolved problem: 0 general integer, 45 binary, 842 continuous\n",
      "Clique table size: 0\n",
      "BRANCHES RELAXS   ACT_NDS  DEPTH    BEST_INT_OBJ         BEST_RELAX_OBJ       REL_GAP(%)  TIME  \n",
      "0        1        1        0        NA                   -2.2004741388e+01    NA          0.1   \n",
      "Cut generation started.\n",
      "0        1        1        0        NA                   -2.2004741388e+01    NA          0.2   \n",
      "0        2        1        0        NA                   -1.2389856582e+01    NA          0.2   \n",
      "0        3        1        0        NA                   -9.0684894182e+00    NA          0.3   \n",
      "0        4        1        0        NA                   -7.4607306102e+00    NA          0.3   \n",
      "0        5        1        0        NA                   -5.7175945211e+00    NA          0.4   \n",
      "0        6        1        0        NA                   -5.0267491555e+00    NA          0.4   \n",
      "0        7        1        0        NA                   -4.7897459707e+00    NA          0.5   \n",
      "0        8        1        0        NA                   -4.5185077428e+00    NA          0.5   \n",
      "0        9        1        0        NA                   -4.2063354604e+00    NA          0.6   \n",
      "0        10       1        0        NA                   -4.1199586212e+00    NA          0.6   \n",
      "0        11       1        0        NA                   -4.0643826396e+00    NA          0.7   \n",
      "0        12       1        0        NA                   -4.0441131247e+00    NA          0.8   \n",
      "0        13       1        0        NA                   -4.0361477959e+00    NA          0.8   \n",
      "0        14       1        0        NA                   -4.0211944373e+00    NA          0.9   \n",
      "Cut generation terminated. Time = 0.37\n",
      "0        16       1        0        NA                   -4.0178531403e+00    NA          0.9   \n",
      "10       27       9        1        NA                   -3.8550985279e+00    NA          1.1   \n",
      "21       38       18       6        NA                   -3.8550985279e+00    NA          1.2   \n",
      "35       52       28       2        NA                   -2.9835961354e+00    NA          1.2   \n",
      "59       76       50       5        NA                   -2.5193430603e+00    NA          1.3   \n",
      "107      124      80       8        NA                   -2.5193430603e+00    NA          1.4   \n",
      "187      204      132      3        NA                   -2.4871454673e+00    NA          1.6   \n",
      "315      331      196      5        6.8927083365e+00     -2.4345703152e+00    135.32      2.0   \n",
      "507      514      258      23       3.1230974294e+00     -1.8971359420e+00    160.75      2.5   \n",
      "763      729      202      26       2.6121076336e+00     -1.0100582224e+00    138.67      3.0   \n",
      "963      887      120      12       2.6121076336e+00     3.3247078051e-01     87.27       3.5   \n",
      "1080     948      19       10       2.6121076336e+00     1.1703393928e+00     55.20       3.7   \n",
      "An optimal solution satisfying the relative gap tolerance of 1.00e-02(%) has been located.\n",
      "The relative gap is 0.00e+00(%).\n",
      "An optimal solution satisfying the absolute gap tolerance of 0.00e+00 has been located.\n",
      "The absolute gap is 0.00e+00.\n",
      "\n",
      "Objective of best integer solution : 2.612107633583e+00      \n",
      "Best objective bound               : 2.612107633583e+00      \n",
      "Construct solution objective       : Not employed\n",
      "User objective cut value           : Not employed\n",
      "Number of cuts generated           : 349\n",
      "  Number of Gomory cuts            : 136\n",
      "  Number of CMIR cuts              : 213\n",
      "Number of branches                 : 1096\n",
      "Number of relaxations solved       : 951\n",
      "Number of interior point iterations: 15\n",
      "Number of simplex iterations       : 9849\n",
      "Time spend presolving the root     : 0.03\n",
      "Time spend optimizing the root     : 0.06\n",
      "Mixed integer optimizer terminated. Time: 3.68\n",
      "\n",
      "Optimizer terminated. Time: 3.71    \n",
      "\n",
      "\n",
      "Integer solution solution summary\n",
      "  Problem status  : PRIMAL_FEASIBLE\n",
      "  Solution status : INTEGER_OPTIMAL\n",
      "  Primal.  obj: 2.6121076336e+00    nrm: 2e+01    Viol.  con: 4e-15    var: 2e-16    itg: 2e-16  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.612107633582955"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_m.solve(solver=cp.GUROBI, verbose=True) # Solve masked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver_name': 'MOSEK',\n",
       " 'solve_time': 3.263864040374756,\n",
       " 'setup_time': None,\n",
       " 'num_iters': None}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_m.solver_stats.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(1-z_m[0].value.reshape(28,28), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the perturbed image through the trained model as a sanity check to validate the results we found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.5576,  5.7639, -1.5296,  0.4447, -0.5642,  6.1776, -0.6303,  2.0922,\n",
       "          3.0115, -2.8892]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_data = torch.tensor(z[0].value).float()\n",
    "perturbed_data = perturbed_data.view(1,28,-1)\n",
    "model(perturbed_data) # Print perturbed normal model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.4564,   5.1414,  -2.0635,   0.8726,  -0.3550,   4.5803,  -0.4836,\n",
       "           3.0838,   3.3845,  -2.7886]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_data = torch.tensor(z_m[0].value).float()\n",
    "perturbed_data = perturbed_data.view(1,28,-1)\n",
    "new_masked_model(perturbed_data) # Print perturbed masked model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List:\n",
    "- Evalute accuracy of both models on the test set\n",
    "- Find subset of images that fool the pruned model but not the original model or maybe vice versa\n",
    "- Make models with different levels of sparsity and see how that affects the MIP result (difference of prob), MIP running time\n",
    "- Print out all the adversarial examples found by the MIP not just the optimal one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get adversarial image database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DO NOT RUN THIS CELL EVER AGAIN PLEASE FOR THE LOVE OF GOD IT TAKES SO LONG, JUST LOAD THE CSV FILE\n",
    "# Create storage database\n",
    "# Need original image number, original classification, target classification, o.f.v, which model it's from, epsilon\n",
    "epsilon = 0.03\n",
    "data = {'Image Number': [], 'Epsilon': [], 'Model': [], 'Original Classification': [], 'Target Classification': [], \n",
    "        'O.F.V': [], 'Adversarial Image': [], 'Generating Time': []}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    image = X[i:i+1]\n",
    "    initial_bound = ((X[i:i+1] - epsilon).clamp(min=0), (X[i:i+1] + epsilon).clamp(max=1))\n",
    "    bounds_m = bound_propagation(new_masked_model, initial_bound)\n",
    "    bounds = bound_propagation(model, initial_bound) \n",
    "    cor_label = y[i].item()\n",
    "    params = {'MSK_DPAR_OPTIMIZER_MAX_TIME': 60}\n",
    "    \n",
    "    for j in range(0,10):\n",
    "        if j != cor_label:\n",
    "            c = np.zeros(10)        \n",
    "            c[cor_label] = 1 \n",
    "            c[j] = -1\n",
    "            \n",
    "            prob, (z, v) = form_milp(model, c, initial_bound, bounds)\n",
    "            prob.solve(solver=cp.MOSEK, mosek_params = params, verbose=False)\n",
    "            df = df.append(pd.DataFrame.from_dict(\n",
    "                {'Image Number': [i], 'Epsilon': [epsilon], 'Model': ['Original'], \n",
    "                 'Original Classification': [cor_label],\n",
    "                 'Target Classification': [j], 'O.F.V': [prob.value],\n",
    "                 'Adversarial Image': [z[0].value],\n",
    "                 'Generating Time': [prob.solver_stats.solve_time]}))\n",
    "            \n",
    "            prob_m, (z_m, v_m) = form_milp(new_masked_model, c, initial_bound, bounds_m)\n",
    "            prob_m.solve(solver=cp.MOSEK, mosek_params = params, verbose=False)\n",
    "            df = df.append(pd.DataFrame.from_dict(\n",
    "                {'Image Number': [i], 'Epsilon': [epsilon], 'Model': ['Pruned'], \n",
    "                 'Original Classification': [cor_label],\n",
    "                 'Target Classification': [j], 'O.F.V': [prob_m.value],\n",
    "                 'Adversarial Image': [z_m[0].value],\n",
    "                 'Generating Time': [prob_m.solver_stats.solve_time]}))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Number</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Model</th>\n",
       "      <th>Original Classification</th>\n",
       "      <th>Target Classification</th>\n",
       "      <th>O.F.V</th>\n",
       "      <th>Adversarial Image</th>\n",
       "      <th>Generating Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Original</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.534143</td>\n",
       "      <td>[0.029999999329447746, 0.0, 0.0299999993294477...</td>\n",
       "      <td>2.263241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Pruned</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.964636</td>\n",
       "      <td>[0.029999999329447746, 0.0, 0.0299999993294477...</td>\n",
       "      <td>2.460373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Original</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.214301</td>\n",
       "      <td>[0.0, 0.029999999329447746, 0.0299999993294477...</td>\n",
       "      <td>4.395417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Pruned</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.591112</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>3.337015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Original</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.081009</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>2.846078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Pruned</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.117254</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>3.041679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Original</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.925384</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>3.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Pruned</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.175285</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>4.595986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Original</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.543757</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>3.660618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Pruned</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.366481</td>\n",
       "      <td>[0.029999999329447746, 0.029999999329447746, 0...</td>\n",
       "      <td>5.897085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Image Number  Epsilon     Model  Original Classification  \\\n",
       "0            0.0     0.03  Original                      7.0   \n",
       "0            0.0     0.03    Pruned                      7.0   \n",
       "0            0.0     0.03  Original                      7.0   \n",
       "0            0.0     0.03    Pruned                      7.0   \n",
       "0            0.0     0.03  Original                      7.0   \n",
       "..           ...      ...       ...                      ...   \n",
       "0           99.0     0.03    Pruned                      9.0   \n",
       "0           99.0     0.03  Original                      9.0   \n",
       "0           99.0     0.03    Pruned                      9.0   \n",
       "0           99.0     0.03  Original                      9.0   \n",
       "0           99.0     0.03    Pruned                      9.0   \n",
       "\n",
       "    Target Classification      O.F.V  \\\n",
       "0                     0.0   6.534143   \n",
       "0                     0.0   8.964636   \n",
       "0                     1.0   7.214301   \n",
       "0                     1.0   8.591112   \n",
       "0                     2.0   6.081009   \n",
       "..                    ...        ...   \n",
       "0                     6.0  12.117254   \n",
       "0                     7.0   5.925384   \n",
       "0                     7.0  -2.175285   \n",
       "0                     8.0   7.543757   \n",
       "0                     8.0   2.366481   \n",
       "\n",
       "                                    Adversarial Image  Generating Time  \n",
       "0   [0.029999999329447746, 0.0, 0.0299999993294477...         2.263241  \n",
       "0   [0.029999999329447746, 0.0, 0.0299999993294477...         2.460373  \n",
       "0   [0.0, 0.029999999329447746, 0.0299999993294477...         4.395417  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         3.337015  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         2.846078  \n",
       "..                                                ...              ...  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         3.041679  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         3.472800  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         4.595986  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         3.660618  \n",
       "0   [0.029999999329447746, 0.029999999329447746, 0...         5.897085  \n",
       "\n",
       "[1800 rows x 8 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as a csv so we don't lose it\n",
    "from IPython.display import FileLink, FileLinks\n",
    "df.to_csv('./adv_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
